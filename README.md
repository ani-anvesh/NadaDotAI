# NadaDotAI ğŸµ â€“ LSTM Music Generator

This project uses a Long Short-Term Memory (LSTM) neural network to generate music based on MIDI data. The model learns patterns from a set of training MIDI files and then produces new compositions by predicting note sequences.

---

## ğŸ§  Project Workflow

1. **Extract MIDI Notes** â€“ Parses `.mid` files to get sequences of notes and chords.
2. **Preprocess Sequences** â€“ Converts notes into integer sequences and prepares input/output for the LSTM.
3. **Train LSTM Model** â€“ Builds and trains an LSTM on the sequences.
4. **Generate New Music** â€“ Uses trained model to predict new sequences of notes.
5. **Convert to MIDI** â€“ Outputs a new `.mid` file from the predicted notes.
6. **(Optional) Streamlit UI** â€“ Adds a web-based interface to interact with the generator.

---

## ğŸ“ Folder Structure

```
NadaDotAI/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                # Original MIDI files
â”‚   â””â”€â”€ processed/          # Pickled note sequences
â”œâ”€â”€ models/                 # Saved models (.h5, mappings)
â”œâ”€â”€ notebooks/              # Prototyping notebooks
â”œâ”€â”€ outputs/                # Generated MIDI files
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ preprocess.py
â”‚   â”œâ”€â”€ model.py
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ generate.py
â”‚   â””â”€â”€ midi_utils.py
â”œâ”€â”€ app/                    # Streamlit app
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ”® Future Enhancements

- [ ] **Real-time Music Generation** â€“ Predict and stream notes live with `mido` or `pygame.midi`.
- [ ] **Transformer Model** â€“ Replace LSTM with Music Transformer for longer context generation.
- [ ] **User-provided MIDI Upload** â€“ Allow users to upload their own MIDI and train/generate from it.
- [ ] **Genre Conditioning** â€“ Let model learn based on genre tags and generate in specific styles.
- [ ] **Streamlit App** â€“ Upload file, generate music, and preview or download `.mid` output via UI.
- [ ] **Keyboard Input** â€“ Play a short melody and let the model continue it.
- [ ] **Visualizer** â€“ Display generated notes using `music21` or MIDI visualization tools.
- [ ] **Cloud Deploy (HuggingFace/Render)** â€“ Make it public-facing with a persistent backend.

---

## ğŸš€ Getting Started

1. Place MIDI files inside `data/raw/`
2. Run `train_model()` from `src/train.py`
3. Generate with `generate_notes()` from `src/generate.py`
4. Convert output to MIDI with `notes_to_midi()` from `src/midi_utils.py`

---

## ğŸ§° Requirements

- Python 3.8+
- TensorFlow
- music21
- numpy
- pickle
- mido (for future real-time playback)
- streamlit (for optional UI)

---

## ğŸ“ Author

NadaDotAI | Built with love and melody ğŸ’™ğŸ¼
